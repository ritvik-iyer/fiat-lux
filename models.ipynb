{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Text-Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from datascience import percentile\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.read_csv('model-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit to Susan Li on Towards Data Science blog post\n",
    "\n",
    "symbols_1 = re.compile('[/(){}\\[\\]\\|@,;]') \n",
    "symbols_2 = re.compile('[^0-9a-z #+_]')\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\" Takes in a string and returns cleaned string\"\"\"\n",
    "    text = text.lower()\n",
    "    text = symbols_1.sub(' ', text)\n",
    "    text = symbols_2.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in stopwords_set)\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test preprocessing step:\n",
    "\n",
    "preprocess(frame['content'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many total words do we have? \n",
    "frame['content'].apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Does breitbart or nytimes articles, on average, contain more words? Would this bias our analysis if we use CountVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's apply the pre-processings step to our articles\n",
    "frame['content'] = frame['content'].apply(preprocess)\n",
    "frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now let's calculate the average number of words per outlet aricle\n",
    "def get_length(text):\n",
    "    words = text.split(' ')\n",
    "    return len(words)\n",
    "\n",
    "tester = frame.copy()\n",
    "tester['num_words'] = tester['content'].apply(get_length)\n",
    "tester.groupby('domain').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looks like nyt articles on average have more words than breitbart articles. This is good to know for future analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last step before modelling: Getting the data in the proper format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = [1 if text == 'breitbart' else 0 for text in frame['domain']]\n",
    "data = frame.drop(columns = 'domain')\n",
    "data['label'] = encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data['content'].values\n",
    "y = data['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the tf-idf vectorizer to encode our text as numerical vectors \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english', max_features = tester.groupby('domain').max()['num_words']['nytimes'])\n",
    "vectorizer.fit(list(data['content'].values))\n",
    "X_train_cv = vectorizer.transform(X_train)\n",
    "X_test_cv = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the dimensions of data matrix X_train and X_test? \n",
    "print(X_train_cv.shape)\n",
    "print(X_test_cv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #1: Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_classifier = MultinomialNB().fit(X_train_cv, y_train)\n",
    "y_pred = nb_classifier.predict(X_test_cv)\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred, target_names=['breitbart', 'nytimes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), index = [i for i in ['breitbart', 'nytimes']],\n",
    "                  columns = [i for i in ['breitbart', 'nytimes']])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, fmt='g')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Naive Bayes Classifier');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot a ROC Curve \n",
    "plt.figure(figsize = (10, 7))\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_probs = nb_classifier.predict_proba(X_test_cv)\n",
    "nb_probs = y_probs[:, 1]\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "nb_auc = roc_auc_score(y_test, nb_probs)\n",
    "\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Naive Bayes: ROC AUC=%.3f' % (nb_auc))\n",
    "\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "nb_fpr, nb_tpr, _ = roc_curve(y_test, nb_probs)\n",
    "\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "plt.plot(nb_fpr, nb_tpr, marker='.', label='Naive Bayes')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #2: Linear Kernel Support Vector Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "linsvm_classifier = SGDClassifier(loss = 'hinge', penalty = 'l2', tol = None, max_iter = 1000)\n",
    "linsvm_classifier.fit(X_train_cv, y_train)\n",
    "y_pred = linsvm_classifier.predict(X_test_cv)\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred, target_names=['breitbart', 'nytimes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), index = [i for i in ['breitbart', 'nytimes']],\n",
    "                  columns = [i for i in ['breitbart', 'nytimes']])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, fmt='g')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Linear Kernel SVM Classifier');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #3: Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train_cv, y_train)\n",
    "y_pred = rf_classifier.predict(X_test_cv)\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred, target_names=['breitbart', 'nytimes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), index = [i for i in ['breitbart', 'nytimes']],\n",
    "                  columns = [i for i in ['breitbart', 'nytimes']])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, fmt='g')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Random Forest Classifier');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #4: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "xg_classifier = GradientBoostingClassifier()\n",
    "xg_classifier.fit(X_train_cv, y_train)\n",
    "y_pred = xg_classifier.predict(X_test_cv)\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred, target_names=['breitbart', 'nytimes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), index = [i for i in ['breitbart', 'nytimes']],\n",
    "                  columns = [i for i in ['breitbart', 'nytimes']])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, fmt='g')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Gradient Boost Classifier');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #5: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg_classifier = LogisticRegression()\n",
    "logreg_classifier.fit(X_train_cv, y_train)\n",
    "y_pred = logreg_classifier.predict(X_test_cv)\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred, target_names=['breitbart', 'nytimes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = pd.DataFrame(confusion_matrix(y_test, y_pred), index = [i for i in ['breitbart', 'nytimes']],\n",
    "                  columns = [i for i in ['breitbart', 'nytimes']])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, fmt='g')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Logistic Regression Classifier');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot a ROC Curve \n",
    "plt.figure(figsize = (10, 7))\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_probs = logreg_classifier.predict_proba(X_test_cv)\n",
    "logreg_probs = y_probs[:, 1]\n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "ns_auc = roc_auc_score(y_test, ns_probs)\n",
    "logreg_auc = roc_auc_score(y_test, logreg_probs)\n",
    "\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Logistic Regression: ROC AUC=%.3f' % (logreg_auc))\n",
    "\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n",
    "logreg_fpr, logreg_tpr, _ = roc_curve(y_test, logreg_probs)\n",
    "\n",
    "plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "plt.plot(logreg_fpr, logreg_tpr, marker='.', label='Logistic Regression')\n",
    "# axis labels\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The top models in decreasing order were: Linear Kernel SVM, Logistic Regression, Random Forest, XGBoost, Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get a more precise idea of the test accuracy, let's get 95% Confidence Intervals for the top 3 models. We will do this through bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_CI(model, X_data, y_data, num_repetitions, percent_confidence):\n",
    "    \"\"\"\n",
    "    Steps:\n",
    "    1. Iterate num_repetitions times \n",
    "    2. In each iteration, fit a new model to a new training set\n",
    "    3. Generate new predictions based on a new test set\n",
    "    4. Generate an accuracy score \n",
    "    5. Return 95% CI of accuracy scores \n",
    "    \"\"\"\n",
    "    bootstrap_statistics = np.array([])\n",
    "    counter = 1\n",
    "    for _ in np.arange(num_repetitions):\n",
    "        print('This is the {}th iteration'.format(counter))\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.25)\n",
    "        vectorizer = TfidfVectorizer(stop_words = 'english', max_features = tester.groupby('domain').max()['num_words']['nytimes'])\n",
    "        vectorizer.fit(list(frame['content'].values))\n",
    "        X_train_cv = vectorizer.transform(X_train)\n",
    "        X_test_cv = vectorizer.transform(X_test)\n",
    "        classifier = model()\n",
    "        classifier.fit(X_train_cv, y_train)\n",
    "        y_predictions = classifier.predict(X_test_cv)\n",
    "        score = np.mean(y_predictions == y_test)\n",
    "        bootstrap_statistics = np.append(bootstrap_statistics, score)\n",
    "        counter += 1\n",
    "    low_end = ((100 - percent_confidence)/2)\n",
    "    high_end = 100 - low_end\n",
    "    low_score = percentile(low_end, bootstrap_statistics)\n",
    "    high_score = percentile(high_end, bootstrap_statistics)\n",
    "    return np.array([low_score, high_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_CI(LogisticRegression, X, y, 100, 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_CI(RandomForestClassifier, X, y, 100, 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_CI(SGDClassifier, X, y, 100, 95)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
